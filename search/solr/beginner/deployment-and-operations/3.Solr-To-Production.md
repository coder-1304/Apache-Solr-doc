# Requirements: 

- You need 2n+1 servers to survive n faliures

**Sever resources:**
- Use SSDs if you can (disk throughput is important)
- 8 CPU cores or better


**Zookeeper:**
- in the bin/solr.in.cmd file we define list of zookeepers in the `REM set ZK_HOST=` via zookeeper hosts seperated by `,`.

# Fine-Tune Your Production Setup

## Dynamic Defaults for ConcurrentMergeScheduler

**Merge Scheduler**
- In Solr, when data is indexed, it's stored in small segments. Periodically, these segments need to be merged to optimize search performance.
- The default method for managing these merges is called ConcurrentMergeScheduler, which uses multiple threads to merge segments in the background.

**Dynamic Defaults**
- The behavior of ConcurrentMergeScheduler can be customized in the Solr configuration file (`solrconfig.xml`).
- By default, this scheduler adjusts its settings based on whether your storage drive is a traditional hard disk (rotational) or a solid-state drive (SSD).

**Auto-Detection**
- If your drive is rotational (like a traditional hard disk), the scheduler limits the number of threads used for merging to 1 and the maximum number of merges to 6, to prevent performance issues.
- If your drive is an SSD or non-rotational, it sets the number of threads to 4 or half the number of CPU cores available to Solr (whichever is greater), and adjusts the maximum number of merges accordingly.

**Platform Limitations**
- The auto-detection feature works reliably only on Linux systems. On other platforms, drives are assumed to be rotational, potentially leading to incorrect settings.
- Incorrect settings can severely impact indexing performance, causing slower operations.

**Manual Configuration**
- To ensure optimal performance, it's recommended to manually set the values for `maxThreadCount` and `maxMergeCount` in the Solr configuration file based on your hardware specifications.
- This way, you can override the auto-detected defaults and tailor the settings to match your specific hardware setup.

**Override Options**
- If needed, you can override the auto-detected values using system properties.
- For example, you can set `lucene.cms.override_spins` to `true/false` to force rotational/non-rotational disk detection, or `lucene.cms.override_core_count` to specify the number of CPU cores manually.


## Memory and GC Settings

By default, the `bin/solr` script sets the maximum Java heap size to 512M (`-Xmx512m`), suitable for initial Solr setup. For production, adjust the maximum heap size based on your application's memory needs. Typically, values between 8 and 16 gigabytes are used for production servers. To modify memory settings, use the `SOLR_HEAP` variable in the include file:

```bash
SOLR_HEAP="8g"
```

Avoid allocating a very large Java heap unless necessary. 

Additionally, the Solr Control Script includes pre-configured Garbage First Garbage Collection settings known to work well with Solr for various workloads. However, these settings may not suit your specific Solr usage. Adjust the GC settings using the GC_TUNE variable in the /etc/default/solr.in.sh include file.


## Going to Production with SolrCloud


When you're ready to start using SolrCloud for production, you'll need to make a few configurations.

First, you have to specify where your ZooKeeper ensemble is located. This is done by setting the ZK_HOST variable in a configuration file. For example, if your ZooKeeper ensemble is hosted on three servers named zk1, zk2, and zk3, you would set:
```
ZK_HOST=zk1,zk2,zk3
```
 </t> Once this is set, Solr will operate in "cloud" mode.

If you're sharing ZooKeeper with other systems, it's a good idea to isolate SolrCloud's data using ZooKeeper's chroot feature. For example, you can add "/solr" to the end of your ZK_HOST connection string:
```
ZK_HOST=zk1,zk2,zk3/solr
```
  Before using chroot for the first time, you'll need to create the root path (znode) in ZooKeeper using the Solr Control Script:
```
bin/solr zk mkroot /solr -z <ZK_node>:<ZK_PORT>
```
  Alternatively, you can use zkcli.sh or zkcli.bat to bootstrap ZooKeeper with an existing solr_home, which will also create the chroot path if needed.

Lastly, it's recommended to set the hostname of your Solr server using the SOLR_HOST variable in the configuration file:
```
SOLR_HOST=solr1.example.com
```
This is especially important in SolrCloud mode, as it determines the address of the node when it registers with ZooKeeper.


## Override Settings in solrconfig.xml

Solr allows you to modify its settings by overriding configuration properties using Java system properties. You can pass these properties at startup using the `-Dproperty=value syntax`.

In the `solrconfig.xml` file, there are default settings for auto soft commits, like:

```
<autoSoftCommit>
  <maxTime>${solr.autoSoftCommit.maxTime:-1}</maxTime>
</autoSoftCommit>
```

Whenever you encounter a property in a Solr configuration file that follows the `${solr.PROPERTY:DEFAULT_VALUE}` syntax, it means you can override it using a Java system property.

For example, if you want to set the maximum time for soft commits to 10 seconds, you can start Solr with `-Dsolr.autoSoftCommit.maxTime=10000`, like this:

```
bin/solr start -Dsolr.autoSoftCommit.maxTime=10000
```

The `bin/solr` script forwards options beginning with `-D` to the JVM during startup. For production use, it's recommended to set these properties in the `SOLR_OPTS` variable defined in the include file.

For instance, let's say you want to adjust the soft-commit example. In `/etc/default/solr.in.sh`, you would add:

```
SOLR_OPTS="$SOLR_OPTS -Dsolr.autoSoftCommit.maxTime=10000"
```


## Ulimit Settings for *nix Operating Systems

On *nix operating systems like Linux or Unix, there are some settings called ulimits that control how much system resources processes can use. It's important to keep an eye on these settings and set them as high as possible, ideally to "unlimited" if your system allows it.

You can check the current values of these settings by typing `ulimit -a` at a command prompt.

Here are the main ulimit settings you should pay attention to and set as high as possible:

- **Max Processes (`ulimit -u`):** This determines the maximum number of processes that can run simultaneously. It's recommended to set this to at least 65,000.

- **File Handles (`ulimit -n`):** This controls the maximum number of files that can be open at once. Since all the files used by Solr replicas need to have their file handles open, it's important to set this to at least 65,000.

- **Virtual Memory (`ulimit -v`):** This setting controls the maximum amount of virtual memory that can be used. It should be set to unlimited since it's used by Solr for MMapping the indexes.

- **Max Memory Size (`ulimit -m`):** Also used by MMap, it should be set to unlimited.

Additionally, if your system supports it, you should set `vm.max_map_count` to unlimited as well.

It's a good idea to permanently raise these settings. The exact process for doing this varies depending on your operating system. You may need to edit configuration files and restart your server. Your system administrators can help you with this.

Make sure to check these limits every time you upgrade your kernel or operating system because these operations can reset them to their defaults.

If these limits are exceeded, Solr may encounter various errors like "too many open files", "connection error", or "max processes exceeded", which can lead to SolrCloud recovery failures.


## Avoiding Swapping on *nix Operating Systems

Avoiding swapping on *nix operating systems, especially when running Java applications like Lucene/Solr, is crucial for maintaining system stability and performance. Swapping memory to disk can severely degrade performance, causing timeouts and instability, particularly in Solr nodes.

> `Swapping` means that the OS is moving less frequently used data from RAM to the hard disk or SSD to free up space in RAM for other processes that need it.

Here are our recommendations for managing swap on Linux systems:

### Disabling Swap:

To temporarily disable swap, use the `swapoff` command:
```bash
sudo swapoff -a
```

For a permanent solution, ensure your system has sufficient physical RAM, and then follow your Linux system's documentation for disabling swap correctly.

### Reducing Swappiness:
Swappiness controls how aggressively the system swaps memory to disk. By default, Linux systems tend to swap out memory aggressively.
To check the current swappiness setting, run:

```bash
cat /proc/sys/vm/swappiness
```

To permanently reduce swappiness, edit /etc/sysctl.conf as root and add or modify the line:

```bash
vm.swappiness = 1
```
Alternatively, you can temporarily change the setting with:

```bash
echo 1 > /proc/sys/vm/swappiness
```
> These steps can help improve system stability and performance, especially in environments running Solr, where swapping can have severe consequences. Additionally, when running Solr within a Docker container, remember to apply these changes to the host system.


